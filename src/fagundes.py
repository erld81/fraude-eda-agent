import streamlit as st
import pandas as pd
import zipfile
import os
import google.generativeai as genai
import matplotlib.pyplot as plt
import io
import contextlib
import unicodedata
import hashlib
import faiss
import numpy as np
import pickle
import tempfile
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from reportlab.lib.pagesizes import A4
from reportlab.lib.units import inch
from reportlab.platypus import Table, TableStyle
from reportlab.lib import colors
# Importa√ß√£o da SentenceTransformer ser√° feita via st.cache_resource

# --- Configura√ß√µes Iniciais e Vari√°veis ---
# Streamlit Page Configuration
st.set_page_config(
    page_title="An√°lise Explorat√≥ria de Dados (EDA) com Gemini e RAG",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Constantes
CHUNK_SIZE = 1000

# --- Inicializa√ß√£o de Session State ---
if 'gemini_api_key' not in st.session_state:
    st.session_state['gemini_api_key'] = ''
if 'zip_bytes' not in st.session_state:
    st.session_state['zip_bytes'] = None
if 'zip_hash' not in st.session_state:
    st.session_state['zip_hash'] = None
    
# CORRE√á√ÉO DO KeyError: "available_files"
if 'available_files' not in st.session_state:
    st.session_state['available_files'] = [] 
    
if 'file_options_map' not in st.session_state:
    st.session_state['file_options_map'] = {}
if 'selected_file_name' not in st.session_state:
    st.session_state['selected_file_name'] = None
if 'df' not in st.session_state:
    st.session_state['df'] = None
if 'df_columns' not in st.session_state:
    st.session_state['df_columns'] = None
if 'faiss_index' not in st.session_state:
    st.session_state['faiss_index'] = None
if 'documents' not in st.session_state:
    st.session_state['documents'] = []
if 'total_lines' not in st.session_state:
    st.session_state['total_lines'] = 0
if 'processed_percentage' not in st.session_state:
    st.session_state['processed_percentage'] = 0
if 'current_chunk_start' not in st.session_state:
    st.session_state['current_chunk_start'] = 0
if 'cleaned_status' not in st.session_state:
    st.session_state['cleaned_status'] = {}
if 'file_name_context' not in st.session_state:
    st.session_state['file_name_context'] = ""
if 'conclusoes_historico' not in st.session_state:
    st.session_state['conclusoes_historico'] = ""
if 'codigo_gerado' not in st.session_state:
    st.session_state['codigo_gerado'] = None
if 'resultado_texto' not in st.session_state:
    st.session_state['resultado_texto'] = None
if 'resultado_df' not in st.session_state:
    st.session_state['resultado_df'] = None
if 'erro_execucao' not in st.session_state:
    st.session_state['erro_execucao'] = None
if 'img_bytes' not in st.session_state:
    st.session_state['img_bytes'] = None
if 'consultar_ia' not in st.session_state:
    st.session_state['consultar_ia'] = False
if 'exibir_codigo' not in st.session_state:
    st.session_state['exibir_codigo'] = False
if 'habilitar_grafico' not in st.session_state:
    st.session_state['habilitar_grafico'] = False
if 'gerar_pdf' not in st.session_state:
    st.session_state['gerar_pdf'] = False
if 'user_query_input_widget' not in st.session_state:
    st.session_state['user_query_input_widget'] = ""
if 'current_query_text' not in st.session_state:
    st.session_state['current_query_text'] = ""


# --- Helper Functions ---
def normalize_text(text):
    """Normaliza texto (remove acentos e cedilhas)."""
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')

# --- Agente 0: Clarifica√ß√£o de Consultas (NOVO AGENTE) ---
def agente0_clarifica_pergunta(pergunta_original, api_key):
    """Usa o Gemini para corrigir erros de digita√ß√£o e clarificar a inten√ß√£o."""
    if not api_key:
        return pergunta_original

    try:
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        prompt = f"""
# INSTRU√á√ïES:
Voc√™ √© um Clarificador de Consultas. Sua √∫nica fun√ß√£o √© corrigir erros de digita√ß√£o e tornar a consulta do usu√°rio o mais clara e objetiva poss√≠vel, SEM alterar o significado original. Sua sa√≠da DEVE ser APENAS a consulta corrigida/clarificada.

# EXEMPLOS DE CORRE√á√ÉO:
USU√ÅRIO: "Qual o tipu de cada colna?"
SA√çDA: "Qual o tipo de cada coluna?"

USU√ÅRIO: "ttata o arquiu?"
SA√çDA: "Do que se trata o arquivo?"
        
# CONSULTA ORIGINAL DO USU√ÅRIO:
{pergunta_original}

# CONSULTA CLARIFICADA:
"""
        response = model.generate_content(prompt)
        # Limita para garantir que seja apenas uma frase
        return response.text.strip().split('\n')[0]
    
    except Exception:
        # Em caso de erro, retorna a pergunta original para n√£o bloquear o fluxo
        return pergunta_original

# --- RAG Components ---
@st.cache_resource
def load_embedding_model():
    """Carrega o modelo de embedding uma √∫nica vez."""
    from sentence_transformers import SentenceTransformer
    return SentenceTransformer('paraphrase-MiniLM-L6-v2')

def create_faiss_index_for_chunk(chunk):
    """Cria/adiciona a um √≠ndice FAISS para um chunk espec√≠fico."""
    model = load_embedding_model()
    
    # 1. Pr√©-processamento
    docs_chunk = chunk.astype(str).apply(lambda x: ' '.join(x), axis=1).tolist()
    
    # Verifica se h√° documentos para processar
    if not docs_chunk:
        return True

    # 2. Embedding
    embeddings_chunk = model.encode(docs_chunk, show_progress_bar=False)
    
    dimension = embeddings_chunk.shape[1]
    
    # 3. Cria√ß√£o/Adi√ß√£o ao √çndice FAISS
    if st.session_state['faiss_index'] is not None:
        st.session_state['faiss_index'].add(np.array(embeddings_chunk).astype('float32'))
    else:
        index = faiss.IndexFlatL2(dimension)
        index.add(np.array(embeddings_chunk).astype('float32'))
        st.session_state['faiss_index'] = index
        
    # 4. Atualiza Documentos
    if st.session_state['documents'] is None:
        st.session_state['documents'] = []
    st.session_state['documents'].extend(docs_chunk)
    
    return True

def retrieve_context(query, index, documents, top_k=3):
    """Recupera os documentos mais relevantes do √≠ndice FAISS para uma dada consulta."""
    model = load_embedding_model()
    query_embedding = model.encode([query])
    
    if index is None or index.ntotal == 0:
        return ""
    
    # Faiss espera np.float32, ent√£o convertemos a query embedding
    D, I = index.search(np.array(query_embedding).astype('float32'), top_k)
    
    retrieved_docs = [documents[i] for i in I[0] if i < len(documents)]
    
    return "\n".join(retrieved_docs)

def save_progress(file_hash, df, faiss_index, documents, total_lines):
    """Salva o progresso no disco."""
    try:
        if st.session_state.get('selected_file_name'):
            unique_file_hash = hashlib.md5((file_hash + st.session_state['selected_file_name']).encode()).hexdigest()
            
            temp_dir = tempfile.gettempdir()
            
            # Garante que o df n√£o est√° vazio antes de salvar
            if df is not None and not df.empty:
                with open(os.path.join(temp_dir, f"{unique_file_hash}_df.pkl"), "wb") as f:
                    pickle.dump(df, f)
            
            # Garante que o √≠ndice foi criado antes de salvar
            if faiss_index is not None and faiss_index.ntotal > 0:
                faiss.write_index(faiss_index, os.path.join(temp_dir, f"{unique_file_hash}_faiss_index.bin"))
                
            if documents:
                with open(os.path.join(temp_dir, f"{unique_file_hash}_documents.pkl"), "wb") as f:
                    pickle.dump(documents, f)
            
            with open(os.path.join(temp_dir, f"{unique_file_hash}_metadata.txt"), "w") as f:
                f.write(str(total_lines))
                
            return True
        return False
    except Exception as e:
        # st.error(f"Erro ao salvar o progresso: {e}") 
        return False

def load_progress(file_hash, selected_file_name):
    """Carrega o progresso do disco, se existir."""
    try:
        unique_file_hash = hashlib.md5((file_hash + selected_file_name).encode()).hexdigest()
        temp_dir = tempfile.gettempdir()
        
        df_path = os.path.join(temp_dir, f"{unique_file_hash}_df.pkl")
        faiss_path = os.path.join(temp_dir, f"{unique_file_hash}_faiss_index.bin")
        docs_path = os.path.join(temp_dir, f"{unique_file_hash}_documents.pkl")
        meta_path = os.path.join(temp_dir, f"{unique_file_hash}_metadata.txt")

        # Verifica se todos os arquivos essenciais existem
        if os.path.exists(df_path) and os.path.exists(faiss_path) and os.path.exists(docs_path):
            with open(df_path, "rb") as f:
                df = pickle.load(f)
            faiss_index = faiss.read_index(faiss_path)
            with open(docs_path, "rb") as f:
                documents = pickle.load(f)
            
            total_lines = 0
            if os.path.exists(meta_path):
                 with open(meta_path, "r") as f:
                    total_lines = int(f.read())
            
            # Retorna o total de linhas do DF carregado, que √© o n√∫mero real de linhas processadas
            return df, faiss_index, documents, len(df)
        return None, None, None, 0
    except Exception as e:
        # print(f"Erro ao carregar o progresso: {e}")
        return None, None, None, 0

# --- Agente 1: Fun√ß√µes de Processamento de Arquivos ---
def agente1_identifica_arquivos(zip_bytes):
    """
    Identifica todos os arquivos CSV, XLSX e TXT no ZIP e tenta obter cabe√ßalhos.
    """
    files_info = []
    
    with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as z:
        for name in z.namelist():
            if name.startswith('__MACOSX/') or name.endswith('/'):
                continue
                
            ext = os.path.splitext(name)[1].lower()
            
            if ext in ['.csv', '.xlsx', '.txt']:
                try:
                    with z.open(name, 'r') as file_in_zip:
                        data_in_memory = io.BytesIO(file_in_zip.read())
                        
                        header = []
                        if ext == '.csv':
                            temp_df = pd.read_csv(data_in_memory, nrows=0, encoding='utf-8', on_bad_lines='skip', low_memory=False)
                            header = temp_df.columns.tolist()
                        
                        elif ext == '.xlsx':
                            temp_df = pd.read_excel(data_in_memory, nrows=0)
                            header = temp_df.columns.tolist()
                        
                        elif ext == '.txt':
                            data_in_memory.seek(0)
                            # Tentativa de ler a primeira linha para inferir o separador
                            first_line = io.TextIOWrapper(data_in_memory, encoding='utf-8').readline().strip()
                            data_in_memory.seek(0)
                            
                            separator = '\s+'
                            if ',' in first_line:
                                separator = ','
                            elif ';' in first_line:
                                separator = ';'

                            temp_df = pd.read_csv(data_in_memory, nrows=1, encoding='utf-8', sep=separator, engine='python', on_bad_lines='skip', header=None)
                            
                            if temp_df.shape[1] > 0:
                                header = [f"COL_{i+1}" for i in range(temp_df.shape[1])]
                            else:
                                header = ["COL_1"] 
                                
                        file_info = {
                            "name": name,
                            "extension": ext,
                            "header": header,
                            "schema_text": ", ".join(header),
                            "num_cols": len(header)
                        }
                        files_info.append(file_info)
                except Exception as e:
                    pass
            
    return files_info

def agente1_interpreta_contexto_arquivo(api_key, file_info_list):
    """
    Usa o Gemini para descrever o que cada arquivo representa com base no nome e cabe√ßalho.
    """
    if not api_key:
        return {info["name"]: "API Key n√£o configurada para gerar contexto." for info in file_info_list}

    contextos = {}
    try:
        genai.configure(api_key=api_key)
        # MODELO ATUALIZADO PARA gemini-2.5-flash
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        prompt_parts = ["# PERSONA: Voc√™ √© um Analista de Dados S√™nior. Sua √∫nica fun√ß√£o √© INFERIR o CONTE√öDO e CONTEXTO de um arquivo de dados baseado no NOME e CABE√áALHO. D√ä UMA DESCRI√á√ÉO DE UMA √öNICA FRASE CURTA. \n\n# ARQUIVOS PARA AN√ÅLISE:\n"]
        
        for info in file_info_list:
            prompt_parts.append(f"- ARQUIVO: {info['name']} (Colunas: {info['schema_text']})\n")
        
        prompt_parts.append("\n# INFER√äNCIA:\nResponda APENAS com uma lista numerada, onde cada item √© uma descri√ß√£o concisa (uma frase) para o respectivo arquivo, focando no que ele representa. Ex: 'O arquivo representa dados de transa√ß√µes de cart√£o de cr√©dito e a coluna CLASS indica fraude.'\n")
        
        response = model.generate_content("".join(prompt_parts))
        
        descricoes = [line.strip() for line in response.text.split('\n') if line.strip().startswith(('1.', '2.', '3.', '-', '*')) or (len(line.strip()) > 5 and i > 0)]
        
        for i, info in enumerate(file_info_list):
            if i < len(descricoes):
                text = descricoes[i]
                if text and text[0].isdigit() and '.' in text[:3]:
                    text = text.split('.', 1)[-1].strip()
                contextos[info["name"]] = text
            else:
                contextos[info["name"]] = f"Erro na infer√™ncia autom√°tica. Cabe√ßalho: {info['schema_text']}"
                
    except Exception as e:
        for info in file_info_list:
             contextos[info["name"]] = f"Erro na infer√™ncia autom√°tica. Cabe√ßalho: {info['schema_text']}"
            
    return contextos

def agente1_processa_arquivo_chunk(zip_bytes, selected_file_name, start_row, nrows, df_columns, expected_num_cols):
    """
    Processa um chunk do arquivo selecionado (CSV, XLSX, TXT) dentro do ZIP.
    """
    ext = os.path.splitext(selected_file_name)[1].lower()
    
    try:
        with zipfile.ZipFile(io.BytesIO(zip_bytes), "r") as z:
            with z.open(selected_file_name, 'r') as file_in_zip:
                file_bytes_in_memory = io.BytesIO(file_in_zip.read())
                
                chunk = pd.DataFrame()
                
                # Configura√ß√µes de leitura
                skiprows = range(1, start_row + 1) if start_row > 0 else 0
                header = None if start_row > 0 else 'infer'

                # Leitura de CSV
                if ext == '.csv':
                    chunk = pd.read_csv(
                        file_bytes_in_memory,
                        skiprows=skiprows,
                        nrows=nrows,
                        low_memory=False,
                        header=header,
                        encoding='utf-8',
                        on_bad_lines='skip'
                    )
                
                # Leitura de XLSX
                elif ext == '.xlsx':
                    chunk = pd.read_excel(file_bytes_in_memory, header=header, skiprows=skiprows, nrows=nrows)
                        
                # Leitura de TXT
                elif ext == '.txt':
                    # Tenta inferir o separador para leitura do chunk
                    file_bytes_in_memory.seek(0)
                    first_line = io.TextIOWrapper(file_bytes_in_memory, encoding='utf-8', errors='ignore').readline().strip()
                    file_bytes_in_memory.seek(0)
                    
                    separator = '\s+'
                    if ',' in first_line:
                        separator = ','
                    elif ';' in first_line:
                        separator = ';'
                        
                    chunk = pd.read_csv(
                        file_bytes_in_memory,
                        skiprows=skiprows,
                        nrows=nrows,
                        low_memory=False,
                        header=header,
                        encoding='utf-8',
                        sep=separator,
                        engine='python',
                        on_bad_lines='skip'
                    )

                if chunk.empty:
                    return None, "Processamento de todos os lotes conclu√≠do."

                # --- TRATAMENTO DE COLUNAS/ESQUEMA ---
                if start_row == 0:
                    # Captura o cabe√ßalho original (antes da normaliza√ß√£o)
                    if df_columns is None:
                        st.session_state['df_columns'] = chunk.columns
                    expected_num_cols = len(st.session_state['df_columns'])
                    
                
                if df_columns is not None:
                    # Corre√ß√£o de Length Mismatch para chunks subsequentes
                    current_cols = chunk.shape[1]
                    
                    if current_cols < expected_num_cols:
                        for i in range(current_cols, expected_num_cols):
                            chunk[f'TEMP_FILL_{i}'] = pd.NA
                        chunk = chunk.iloc[:, :expected_num_cols]
                    
                    elif current_cols > expected_num_cols:
                        chunk = chunk.iloc[:, :expected_num_cols]
                        
                    # Atribui os nomes de coluna originais (normalizados)
                    chunk.columns = [normalize_text(col.strip().upper()) for col in st.session_state['df_columns']]
                else:
                    # Normaliza√ß√£o das colunas
                    chunk.columns = [normalize_text(col.strip().upper()) for col in chunk.columns]

                return chunk, "Dados carregados e prontos para an√°lise!"
            
    except Exception as e:
        return None, f"Erro ao processar o arquivo: header must be integer or list of integers {e}"

# --- Agente de Limpeza de Dados ---
def agente_limpeza_dados(df):
    """
    Identifica e converte colunas para tipos num√©ricos e categ√≥ricos.
    Aplica a limpeza 'in-place' no DF.
    """
    if df is None:
        return None

    # Iterar sobre uma c√≥pia da lista de colunas para evitar problemas de modifica√ß√£o durante o loop
    for col in list(df.columns):
        if col not in df.columns: # Prote√ß√£o caso a coluna seja exclu√≠da ou renomeada
             continue

        temp_series = pd.to_numeric(df[col], errors='coerce')
        
        # 1. Num√©rico
        if temp_series.notna().sum() / len(temp_series) > 0.8:
            df[col] = temp_series
            st.session_state['cleaned_status'][col] = 'Numeric'
        
        # 2. Categ√≥rico
        elif df[col].nunique() < 50 and len(df[col].unique()) < len(df) / 2:
            df[col] = df[col].astype('category')
            st.session_state['cleaned_status'][col] = 'Categorical'
        
        # 3. Texto/Objeto
        else:
            st.session_state['cleaned_status'][col] = 'Object'
    
    return df

# --- Agente 2 & 4: Gera√ß√£o de C√≥digo e Conclus√£o ---
def agente2_gera_codigo_pandas_eda(pergunta, api_key, df, retrieved_context=None, historico_conclusoes=None, file_context=None):
    """Gera c√≥digo Pandas para EDA e a conclus√£o em linguagem natural."""
    if df is None:
        return "Erro: DataFrame n√£o carregado. Fa√ßa o upload do arquivo primeiro.", None

    if not api_key:
        return "Erro: Chave da API do Gemini n√£o fornecida.", None

    try:
        genai.configure(api_key=api_key)
        # MODELO ATUALIZADO PARA gemini-2.5-flash
        model = genai.GenerativeModel('gemini-2.5-flash')

        schema = '\n'.join([f"- {c} (dtype: {df[c].dtype})" for c in df.columns])

        pergunta_limpa = normalize_text(pergunta).upper()
        
        # 1. PERGUNTAS SOBRE TIPOS/ESQUEMA DE COLUNAS (Gera um DataFrame tabular VERTICAL)
        if any(keyword in pergunta_limpa for keyword in ["QUE TIPO DE DADOS", "QUAIS OS TIPOS DE COLUNAS", "DTYPE COLUNAS", "TIPOS DE DADOS NAS COLUNAS", "TIPOS DAS COLUNAS"]):
            
            codigo_gerado = f"""
# Cria um DataFrame vertical com duas colunas
schema_df = pd.DataFrame(df.dtypes).reset_index()
schema_df.columns = ['NOME_DA_COLUNA', 'TIPO_DE_DADO']

# Atribui para visualiza√ß√£o tabular no Streamlit
resultado_df = schema_df
print(resultado_df.to_string(index=False)) # Imprime sem o √≠ndice para limpeza
"""
            conclusoes = "A an√°lise revela o tipo de dado de cada coluna no conjunto de dados, auxiliando na verifica√ß√£o de consist√™ncia e na prepara√ß√£o para modelagem."
            return codigo_gerado, conclusoes
            
        # 2. PERGUNTAS SOBRE CONTEXTO GERAL (Gera apenas texto que ser√° convertido em tabela 1x1)
        if any(keyword in pergunta_limpa for keyword in ["QUE SE TRATA O ARQUIVO", "CONTEUDO DO ARQUIVO", "REPRESENTA O ARQUIVO", "O QUE E ESSE DATASET"]):
            prompt_interpretacao = f"""
# PERSONA E OBJETIVO PRINCIPAL
Voc√™ √© um Analista de Dados S√™nior. Sua √∫nica fun√ß√£o √© INTERPRETAR e DESCREVER o conte√∫do de um DataFrame. Sua sa√≠da DEVE ser APENAS uma descri√ß√£o textual detalhada (qualitativa), sem c√≥digo ou coment√°rios.

# CONTEXTO DO DATAFRAME `df`
Esquema do DataFrame:
{schema}
CONTEXTO DO NOME DO ARQUIVO: '{file_context}'.

# PERGUNTA DO USU√ÅRIO
{pergunta}

# DESCRI√á√ÉO FINAL DO CONTE√öDO
"""
            response = model.generate_content(prompt_interpretacao)
            texto_limpo = response.text.replace("'", "\\'").replace('"', '\\"').replace('\n', ' ').strip()
            # O executor de c√≥digo ir√° criar um resultado_df a partir deste print para garantir a tabela.
            codigo_gerado = f"print('{texto_limpo}')"
            
            # Conclus√£o customizada para contextualiza√ß√£o (sem contagem errada de linhas)
            conclusoes_contexto = f"O arquivo '{file_context}' foi contextualizado. Ele cont√©m {len(df)} registros."
            
            return codigo_gerado, conclusoes_contexto
            
        # 3. CORRE√á√ÉO ESPEC√çFICA PARA BOXPLOT e HISTOGRAMA (Evita m√∫ltiplas figuras e layout fixo)
        if any(keyword in pergunta_limpa for keyword in ["OUTLIER", "BOXPLOT", "DISPERSAO", "HISTOGRAMA", "DISTRIBUICAO"]):
             
            plot_type = 'boxplot' if any(k in pergunta_limpa for k in ["OUTLIER", "BOXPLOT"]) else 'hist'
            plot_func = 'df.boxplot(column=col, ax=axes[i], grid=False)' if plot_type == 'boxplot' else 'axes[i].hist(df[col].dropna(), bins=20, edgecolor="black")'
            plot_title = 'An√°lise de Outliers - Boxplots para Colunas Num√©ricas' if plot_type == 'boxplot' else 'Distribui√ß√£o de Dados - Histogramas para Colunas Num√©ricas'
            
            codigo_gerado = f"""
import numpy as np

# 1. Identifica colunas num√©ricas
numerical_cols = df.select_dtypes(include=np.number).columns
num_plots = len(numerical_cols)

if num_plots == 0:
    print("N√£o h√° colunas num√©ricas para plotar.")
else:
    # 2. Calcula o layout din√¢mico (max 4 colunas)
    n_cols = min(4, num_plots)
    n_rows = int(np.ceil(num_plots / n_cols))

    # 3. Cria a √∫nica figura principal com o layout din√¢mico
    fig, axes = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(4 * n_cols, 3 * n_rows))
    axes = axes.flatten() # Achata para itera√ß√£o f√°cil

    # 4. Itera e plota
    for i, col in enumerate(numerical_cols):
        # Usa o eixo (axis) do subplot correto
        {plot_func}
        axes[i].set_title(col, fontsize=10)
        axes[i].tick_params(axis='x', rotation=45)

    # 5. Remove eixos vazios (se existirem)
    for j in range(num_plots, n_rows * n_cols):
        fig.delaxes(axes[j])

    plt.suptitle("{plot_title}", y=1.02, fontsize=14)
    plt.tight_layout()
"""
            conclusoes = f"Os gr√°ficos de {plot_type} foram gerados para visualizar a dispers√£o dos dados e identificar potenciais problemas de distribui√ß√£o ou outliers em cada coluna num√©rica do dataset."
            
            return codigo_gerado, conclusoes

        # --- L√ìGICA: GERA√á√ÉO DE C√ìDIGO (RAG - AN√ÅLISE GERAL) ---
        
        rag_context_str = f"\n\nCONTEXTO ADICIONAL DOS DADOS (RAG):\n{retrieved_context}" if retrieved_context else ""
        historico_conclusoes_str = f"\n\nHIST√ìRICO DE AN√ÅLISE E CONCLUS√ïES ANTERIORES:\n{historico_conclusoes}" if historico_conclusoes else ""
        file_context_str = f"\n\nCONTEXTO DO NOME DO ARQUIVO: '{file_context}'."
        
        prompt = f"""
# PERSONA E OBJETIVO PRINCIPAL
Voc√™ √© um assistente especialista em An√°lise Explorat√≥ria de Dados (E.D.A.) com Pandas.
Sua √∫nica fun√ß√£o √© traduzir uma pergunta em linguagem natural para um c√≥digo Python.
Voc√™ DEVE gerar apenas o c√≥digo Python.

# CONTEXTO DO DATAFRAME `df`
Esquema do DataFrame:
{schema}
{file_context_str}
{rag_context_str}
{historico_conclusoes_str}

# REGRAS DE GERA√á√ÉO DE C√ìDIGO (MUITO IMPORTANTE)
1.  **Sempre use `df` como o nome do DataFrame.**
2.  **NUNCA gere c√≥digo para carregar (`pd.read_csv`, `pd.read_excel`, etc.) ou salvar o DataFrame `df`. Ele j√° est√° carregado e pronto para uso.**
3.  **Se o resultado for uma tabela de dados (DataFrame), SEMPRE atribua-o a `resultado_df` e imprima `resultado_df` (ex: `print(resultado_df.to_string())`).**
4.  **Para gr√°ficos, use `matplotlib.pyplot` (importado como `plt`). Para gr√°ficos com m√∫ltiplos subplots, use `plt.subplots()` com layout din√¢mico (`numpy.ceil`).**
5.  **A sa√≠da final deve ser APENAS o c√≥digo Python, sem explica√ß√µes ou coment√°rios, e JAMAIS inclua qualquer pergunta.**
6.  **EVITE usar zero √† esquerda em n√∫meros decimais inteiros (ex: use '8' em vez de '08') para evitar erro de sintaxe 'octal integers'.**

# PERGUNTA DO USU√ÅRIO
{pergunta}

# C√ìDIGO PYTHON (PANDAS/MATPLOTLIB)
"""
        response = model.generate_content(prompt)
        codigo_gerado = response.text.replace("```python", "").replace("```", "").strip()
        
        # Agente 4: GERA AS CONCLUS√ïES AP√ìS A AN√ÅLISE
        conclusoes_prompt = f"""
# PERSONA
Voc√™ √© um analista de dados s√™nior e seu √∫nico trabalho √© sintetizar os resultados de uma an√°lise e fornecer conclus√µes ou insights claros e objetivos para o usu√°rio.

# CONTEXTO
A pergunta do usu√°rio foi: "{pergunta}"
O resultado da an√°lise (C√≥digo Python) foi:
{codigo_gerado}

# TAREFA
Com base na pergunta do usu√°rio e nos resultados, forne√ßa uma ou duas frases de conclus√£o sobre o que foi descoberto. N√£o mencione o c√≥digo. Apenas a conclus√£o.
"""
        conclusoes_response = model.generate_content(conclusoes_prompt)
        conclusoes = conclusoes_response.text

        return codigo_gerado, conclusoes
    except Exception as e:
        return f"Erro ao chamar a API do Gemini: {e}", None

# --- Executor de C√≥digo Seguro ---
def executa_codigo_seguro(codigo, df):
    """Executa o c√≥digo Pandas/Matplotlib gerado em um ambiente isolado."""
    if codigo.startswith("Erro:"):
        return codigo, None, None, None

    output_stream = io.StringIO()
    local_vars = {'df': df, 'pd': pd, 'plt': plt, 'normalize_text': normalize_text, 'np': np} # Adiciona np
    img_bytes = None

    try:
        with contextlib.redirect_stdout(output_stream):
            # Adiciona o df de forma segura para o exec
            local_vars['df'] = df.copy() 
            exec(codigo, {"__builtins__": __builtins__}, local_vars)
        
        # --- L√≥gica Aprimorada de Captura de Gr√°fico ---
        # Captura apenas a primeira figura (esperamos que seja a figura com todos os subplots)
        if len(plt.get_fignums()) > 0:
            for fig_num in plt.get_fignums():
                plt.figure(fig_num)
                buf = io.BytesIO()
                
                # Garante que layout est√° ajustado para subplots
                try:
                    plt.tight_layout()
                except Exception:
                    pass
                    
                plt.savefig(buf, format="png")
                img_bytes = buf.getvalue()
                buf.close()
                plt.close(fig_num)
                break
        
        resultado_texto = output_stream.getvalue().strip()
        resultado_df = local_vars.get('resultado_df')

        # Normaliza Series para DataFrame
        if isinstance(resultado_df, pd.Series):
            resultado_df = resultado_df.reset_index()
            if len(resultado_df.columns) == 2 and 'index' in resultado_df.columns:
                resultado_df.columns = ['Categoria', 'Valor']
        
        # BLOCO CR√çTICO: GARANTE QUE TODO TEXTO SEJA CONVERTIDO EM DATAFRAME (TABELA)
        if resultado_df is None and resultado_texto and not img_bytes:
            # Cria um DataFrame de 1x1 com a resposta textual
            resultado_df = pd.DataFrame({'INFORMA√á√ÉO': [resultado_texto]})
            # Limpa o resultado_texto para que o Streamlit priorize resultado_df
            resultado_texto = ""

        return resultado_texto, resultado_df, None, img_bytes

    except Exception as e:
        error_message = f"Erro ao executar o c√≥digo gerado pela IA:\n\n{e}\n\nC√≥digo que falhou:\n```python\n{codigo}\n```"
        return error_message, None, error_message, None

# --- Agente 3: Formata√ß√£o da Apresenta√ß√£o ---
def agente3_formatar_apresentacao(resultado_texto, resultado_df, pergunta, img_bytes):
    """Gera o relat√≥rio em PDF (ReportLab)."""
    
    pdf_bytes = None
    final_text_output = resultado_texto

    # Usa o DataFrame para texto se estiver dispon√≠vel (para PDF)
    if resultado_df is not None and not resultado_df.empty:
        final_text_output = resultado_df.to_markdown(index=resultado_df.index.name is not None)
    
    try:
        pdf_output_buffer = io.BytesIO()
        c = canvas.Canvas(pdf_output_buffer, pagesize=A4)
        width, height = A4
        
        c.setFont("Helvetica-Bold", 14)
        c.drawString(inch, height - inch, "Relat√≥rio de An√°lise de Dados")
        
        c.setFont("Helvetica", 12)
        textobject = c.beginText(inch, height - inch - 20)
        textobject.textLines(f"Pergunta: {pergunta}")
        c.drawText(textobject)
        
        y_pos = height - inch - 50

        if resultado_df is not None and not resultado_df.empty:
            
            # Adiciona o √≠ndice como primeira coluna se o DataFrame tiver um nome de √≠ndice definido
            if resultado_df.index.name is not None:
                data = [
                    [resultado_df.index.name] + resultado_df.columns.tolist()
                ] + [
                    [str(idx)] + row for idx, row in zip(resultado_df.index.tolist(), resultado_df.values.astype(str).tolist())
                ]
            else:
                data = [resultado_df.columns.tolist()] + resultado_df.values.astype(str).tolist()

            # Limita a largura da tabela
            max_cols = 10 
            data = [row[:max_cols] for row in data]
            
            table = Table(data)
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('BACKGROUND', (0, 1), (-1, -1), colors.white),
                ('GRID', (0, 0), (-1, -1), 1, colors.black)
            ]))
            
            table_width, table_height = table.wrapOn(c, width, height)
            y_pos -= table_height + 20
            
            if y_pos < inch:
                c.showPage()
                y_pos = height - inch - 20
            
            table.drawOn(c, inch, y_pos)
            y_pos -= 20
        
        if img_bytes:
            image_reader = ImageReader(io.BytesIO(img_bytes))
            img_width, img_height = image_reader.getSize()
            
            aspect_ratio = img_width / img_height
            max_width = width - 2 * inch
            
            img_width = max_width
            img_height = img_width / aspect_ratio
            
            y_pos -= img_height + 20
            
            if y_pos < inch:
                c.showPage()
                y_pos = height - inch - 20
            
            c.drawImage(image_reader, inch, y_pos, width=img_width, height=img_height)

        c.save()
        pdf_bytes = pdf_output_buffer.getvalue()

    except Exception as e:
        pdf_bytes = None

    return final_text_output, img_bytes, pdf_bytes

# --- Streamlit UI ---
st.title("An√°lise Explorat√≥ria de Dados (EDA) com Gemini e RAG")
st.markdown("---")


# --- Configura√ß√£o da Sidebar ---
with st.sidebar:
    st.header("Configura√ß√µes")
    with st.form("api_key_form"):
        api_key_input = st.text_input("Cole sua API Key do Gemini aqui", value=st.session_state['gemini_api_key'], type="password", key="gemini_api_key_input_form")
        submitted = st.form_submit_button("Salvar API Key")
        if submitted:
            st.session_state['gemini_api_key'] = api_key_input
            st.success("API Key salva com sucesso!")

    st.markdown("---")
    st.info("1. Fa√ßa o upload do arquivo **ZIP**.")
    st.info("2. Clique em **'Listar Arquivos'** para identifica√ß√£o.")
    st.info("3. Escolha o arquivo e clique em **'Analisar'**.")
    st.info("4. Fa√ßa sua pergunta de EDA.")

# --- Se√ß√£o 1: Upload e Sele√ß√£o de Dados ---
st.header("1. Upload e Sele√ß√£o de Dados")
zipfile_input = st.file_uploader("Selecione o arquivo ZIP com o(s) arquivo(s) de dados (CSV, XLSX, TXT)", type=["zip"])

if st.button("Listar Arquivos no ZIP"):
    if zipfile_input is not None:
        st.session_state['zip_bytes'] = zipfile_input.getvalue()
        st.session_state['zip_hash'] = hashlib.md5(st.session_state['zip_bytes']).hexdigest()
        
        # Reseta estados importantes
        st.session_state['selected_file_name'] = None 
        st.session_state['df'] = None 
        st.session_state['conclusoes_historico'] = "" 
        st.session_state['processed_percentage'] = 0
        st.session_state['available_files'] = []
        st.session_state['file_options_map'] = {}
        st.session_state['faiss_index'] = None
        st.session_state['documents'] = []


        with st.spinner("Analisando arquivos e gerando contexto com Gemini..."):
            file_info_list = agente1_identifica_arquivos(st.session_state['zip_bytes'])
            
            if not file_info_list:
                st.error("O ZIP n√£o cont√©m arquivos CSV, XLSX ou TXT v√°lidos.")
            else:
                st.session_state['available_files'] = file_info_list
                
                file_context_map = agente1_interpreta_contexto_arquivo(st.session_state.get('gemini_api_key'), file_info_list)
                
                options = {}
                for info in file_info_list:
                    context = file_context_map.get(info['name'], "Contexto n√£o gerado.")
                    options[info['name']] = f"**{info['name']}** - {context}"
                
                st.session_state['file_options_map'] = options
                st.success(f"Encontrados {len(file_info_list)} arquivos de dados. Escolha qual analisar.")

if st.session_state['available_files']:
    
    st.markdown("---")
    st.subheader("Selecione o Arquivo para An√°lise:")
    
    options_list = list(st.session_state['file_options_map'].keys())
    
    if not options_list:
        st.error("Nenhum arquivo eleg√≠vel encontrado ap√≥s a identifica√ß√£o.")
    else:
        display_options = [st.session_state['file_options_map'][key] for key in options_list]
        
        default_index = 0
        try:
            if st.session_state['selected_file_name'] in options_list:
                default_index = options_list.index(st.session_state['selected_file_name'])
        except:
             default_index = 0

        selected_display = st.radio(
            "Arquivos Encontrados (Nome - Contexto Inferido):",
            options=display_options,
            index=default_index,
            key="file_selection_radio"
        )
        
        selected_file_name = options_list[display_options.index(selected_display)]
        st.session_state['selected_file_name'] = selected_file_name
        
        selected_file_info = next((info for info in st.session_state['available_files'] if info["name"] == selected_file_name), None)
        
        if st.button(f"Analisar Arquivo: {selected_file_name}") and selected_file_info:
            
            expected_num_cols = selected_file_info['num_cols']
            
            # --- IN√çCIO DO PROCESSO DE CARGA/CHUNKED (RAG) ---
            
            st.info(f"Tentando carregar progresso anterior para **{selected_file_name}**...")
            
            # Tenta carregar o progresso anterior
            df_loaded, index_loaded, docs_loaded, lines_loaded_processed = load_progress(st.session_state['zip_hash'], selected_file_name)
            
            st.session_state['df'] = df_loaded
            st.session_state['faiss_index'] = index_loaded
            st.session_state['documents'] = docs_loaded
            st.session_state['current_chunk_start'] = lines_loaded_processed # Onde deve continuar o chunking
            
            # Tenta obter o total de linhas real do arquivo
            total_lines_file = 0
            try:
                ext = selected_file_info['extension']
                if ext == '.csv' or ext == '.txt':
                    with zipfile.ZipFile(io.BytesIO(st.session_state['zip_bytes']), "r") as z:
                        with z.open(selected_file_name, 'r') as file_in_zip:
                            # Subtrai 1 para o cabe√ßalho
                            total_lines_file = sum(1 for line in io.TextIOWrapper(file_in_zip, encoding='utf-8', errors='ignore')) - 1 
                else:
                    # Para XLSX, apenas usamos uma estimativa inicial alta
                    total_lines_file = CHUNK_SIZE * 50 
                    
            except Exception as e:
                total_lines_file = CHUNK_SIZE * 10 
            
            # Define o total de linhas real do arquivo (ou o que foi processado se for maior que a estimativa)
            st.session_state['total_lines'] = max(total_lines_file, lines_loaded_processed)
            
            st.session_state['file_name_context'] = normalize_text(os.path.splitext(selected_file_name)[0].upper().replace('_', ' ').replace('-', ' '))

            # Verifica se o carregamento foi completo ou se precisa continuar
            if lines_loaded_processed > 0 and lines_loaded_processed >= total_lines_file:
                st.session_state['df'] = agente_limpeza_dados(st.session_state['df'])
                st.session_state['processed_percentage'] = 100
                st.success(f"Processamento de **{selected_file_name}** conclu√≠do (total de linhas: {len(st.session_state['df'])}).")
                progress_bar = st.progress(1.0, text="Processamento finalizado. A ferramenta est√° pronta para uso!")
                st.rerun() 
            
            # Se o carregamento parcial ocorreu, precisamos continuar
            elif lines_loaded_processed > 0 and lines_loaded_processed < total_lines_file:
                st.info(f"Progresso parcial encontrado ({lines_loaded_processed} linhas). Continuaremos o processamento para as {total_lines_file - lines_loaded_processed} linhas restantes.")
                st.session_state['df_columns'] = st.session_state['df'].columns # Garante que as colunas sejam mantidas
                st.session_state['df'] = agente_limpeza_dados(st.session_state['df']) # Limpa a parte j√° carregada
            
            # --- IN√çCIO DO NOVO PROCESSAMENTO (Se o carregamento falhou ou √© a primeira vez) ---
            else:
                st.info(f"Iniciando novo processamento para **{selected_file_name}** ({st.session_state['total_lines']} linhas estimadas)...")
                st.session_state['df'] = None
                st.session_state['faiss_index'] = None
                st.session_state['documents'] = []
                st.session_state['conclusoes_historico'] = ""
                st.session_state['df_columns'] = None
                st.session_state['processed_percentage'] = 0
                st.session_state['cleaned_status'] = {}
                st.session_state['current_chunk_start'] = 0
                lines_loaded_processed = 0


            # Loop de processamento de chunks
            progress_bar = st.progress(lines_loaded_processed / st.session_state['total_lines'], 
                                       text=f"Criando embeddings e √≠ndice RAG... {lines_loaded_processed}/{st.session_state['total_lines']} linhas...")
            
            start_row = lines_loaded_processed
            
            while start_row < st.session_state['total_lines'] or start_row == 0:
                
                chunk_processed, msg = agente1_processa_arquivo_chunk(
                    st.session_state['zip_bytes'], 
                    selected_file_name, 
                    start_row, 
                    CHUNK_SIZE, 
                    st.session_state['df_columns'],
                    expected_num_cols
                )
                
                if chunk_processed is not None:
                    
                    # 1. Aplica limpeza e concatena
                    chunk_processed = agente_limpeza_dados(chunk_processed)
                    
                    if st.session_state['df'] is None:
                        st.session_state['df'] = chunk_processed
                        st.session_state['df_columns'] = chunk_processed.columns
                        # Re-calcula o n√∫mero total de colunas esperado
                        expected_num_cols = len(st.session_state['df_columns'])
                    else:
                        # Garante que as colunas do chunk coincidam com o DF principal
                        if len(chunk_processed.columns) == len(st.session_state['df_columns']):
                            chunk_processed.columns = st.session_state['df_columns']
                        st.session_state['df'] = pd.concat([st.session_state['df'], chunk_processed], ignore_index=True)
                    
                    # 2. Cria √≠ndice RAG para o chunk
                    create_faiss_index_for_chunk(chunk_processed)
                    
                    # 3. Atualiza progresso
                    start_row += len(chunk_processed)
                    st.session_state['current_chunk_start'] = start_row
                    
                    # Recalibra o total de linhas se necess√°rio
                    if len(chunk_processed) < CHUNK_SIZE and start_row < st.session_state['total_lines']:
                         st.session_state['total_lines'] = start_row
                         
                    progress_value = min(start_row / st.session_state['total_lines'], 1.0) if st.session_state['total_lines'] > 0 else 1.0
                    st.session_state['processed_percentage'] = progress_value * 100
                    
                    progress_bar.progress(progress_value, 
                                          text=f"Criando embeddings e √≠ndice RAG... {start_row}/{st.session_state['total_lines']} linhas - {st.session_state['processed_percentage']:.1f}%")
                    
                    save_progress(st.session_state['zip_hash'], st.session_state['df'], st.session_state['faiss_index'], st.session_state['documents'], st.session_state['total_lines'])
                    
                    # Condi√ß√£o de parada (processou o √∫ltimo chunk)
                    if len(chunk_processed) < CHUNK_SIZE:
                        st.session_state['total_lines'] = start_row # Fixa o total de linhas
                        break
                        
                else:
                    if "todos os lotes conclu√≠do" in msg:
                        st.session_state['total_lines'] = start_row # Fixa o total de linhas
                        break
                    st.error(msg)
                    break
            
            if st.session_state['df'] is not None and len(st.session_state['df']) > 0:
                st.session_state['total_lines'] = len(st.session_state['df'])
                st.success(f"Processamento de **{selected_file_name}** conclu√≠do! Total de linhas carregadas: {len(st.session_state['df'])}")
                progress_bar.progress(1.0, text="Processamento finalizado. A ferramenta est√° pronta para uso!")
                st.session_state['processed_percentage'] = 100
                st.rerun() 
            else:
                progress_bar.empty()
                st.error("Falha ao carregar o arquivo. Verifique se o formato est√° correto.")

st.markdown("---")

# --- Se√ß√£o 2: Consulta √† IA ---

st.header("2. Consultar a IA")

if st.session_state['df'] is None or st.session_state['processed_percentage'] < 5.0:
    st.info("Aguardando o processamento do arquivo para habilitar a consulta.")
else:
    if st.session_state['file_name_context'] and st.session_state['selected_file_name']:
        st.info(f"Analisando: **{st.session_state['selected_file_name']}**. Contexto: **{st.session_state['file_name_context']}**.")

    pergunta = st.text_area(
        "Pergunte em portugu√™s sobre os dados:",
        value=st.session_state.get('user_query_input_widget', ""),
        placeholder="Ex: Qual o tipo de cada coluna? Me d√™ as estat√≠sticas descritivas. H√° correla√ß√£o entre V1 e V2? Gere um boxplot para outliers.",
        height=100,
        key="user_query_input_widget"
    )
    
    # Bot√µes de A√ß√£o
    col1_icons, col2_icons, col3_icons, col4_icons, col5_icons = st.columns([1, 1, 1, 1, 4])
    
    if col1_icons.button("Consultar (üîé)"):
        st.session_state['consultar_ia'] = True

    # Bot√µes de display para ativar ap√≥s a consulta (s√≥ aparecem ap√≥s a primeira consulta)
    if st.session_state.get('codigo_gerado'):
        with col2_icons:
            if st.button("Gr√°fico (üìä)"):
                st.session_state['habilitar_grafico'] = True
        with col3_icons:
            if st.button("C√≥digo (‚úçÔ∏è)"):
                st.session_state['exibir_codigo'] = True
        with col4_icons:
            if st.button("PDF (üìÑ)"):
                st.session_state['gerar_pdf'] = True

    st.markdown("---")
    
    # --- L√≥gica de Execu√ß√£o da Consulta (BLOBO ATUALIZADO) ---
    if 'consultar_ia' in st.session_state and st.session_state['consultar_ia']:
        st.session_state['consultar_ia'] = False
        
        if not st.session_state.get('gemini_api_key'):
            st.error("Por favor, insira e salve sua API Key do Gemini na barra lateral.")
        elif st.session_state['faiss_index'] is None or st.session_state['faiss_index'].ntotal == 0:
            st.warning("O √≠ndice RAG n√£o foi criado. Por favor, processe o arquivo (clique em 'Analisar Arquivo' e aguarde o progresso).")
        else:
            pergunta_original = pergunta # Captura a pergunta original do widget
            
            # --- NOVA ETAPA DE CLARIFICA√á√ÉO ---
            with st.spinner("Clarificando sua pergunta e corrigindo poss√≠veis erros de digita√ß√£o..."):
                pergunta_clarificada = agente0_clarifica_pergunta(pergunta_original, st.session_state['gemini_api_key'])
            
            pergunta_para_ia = pergunta_clarificada # A partir daqui, usa a vers√£o limpa
            
            # Exibe a corre√ß√£o se ela ocorreu
            if pergunta_para_ia != pergunta_original:
                 st.warning(f"Sua consulta foi clarificada para: **{pergunta_para_ia}**")
            # --- FIM DA NOVA ETAPA ---
            
            st.info(f"An√°lise realizada sobre **{st.session_state['processed_percentage']:.1f}%** dos dados j√° processados (total de **{len(st.session_state['df'])}** linhas).")
            
            with st.spinner("Gerando c√≥digo e analisando dados..."):
                df_to_use = st.session_state['df']
                faiss_index = st.session_state['faiss_index']
                documents = st.session_state['documents']
                api_key = st.session_state['gemini_api_key']

                # 1. Recupera o Contexto (RAG) - USANDO A PERGUNTA CLARIFICADA
                retrieved_context = retrieve_context(pergunta_para_ia, faiss_index, documents)
                
                # 2. Gera C√≥digo e Conclus√£o - USANDO A PERGUNTA CLARIFICADA
                codigo_gerado, conclusoes = agente2_gera_codigo_pandas_eda(
                    pergunta_para_ia, 
                    api_key, 
                    df_to_use, 
                    retrieved_context, 
                    st.session_state['conclusoes_historico'],
                    st.session_state['file_name_context'] 
                )
                
                if conclusoes:
                    # Adiciona a nova conclus√£o ao hist√≥rico
                    st.session_state['conclusoes_historico'] += f"\n- {conclusoes}"
                
                st.session_state['codigo_gerado'] = codigo_gerado
                
                # 3. Executa o C√≥digo
                if codigo_gerado.startswith("Erro:"):
                    st.error(codigo_gerado)
                else:
                    resultado_texto, resultado_df, erro_execucao, img_bytes = executa_codigo_seguro(codigo_gerado, df_to_use)
                    
                    st.session_state['resultado_texto'] = resultado_texto
                    st.session_state['resultado_df'] = resultado_df
                    st.session_state['erro_execucao'] = erro_execucao
                    st.session_state['img_bytes'] = img_bytes
                    
                    if erro_execucao:
                        st.error(erro_execucao)
                    else:
                        st.subheader("Resultado da An√°lise:")
                        
                        # Exibe Tabela (Corrigido para evitar truncamento em colunas longas)
                        if resultado_df is not None and not resultado_df.empty:
                            
                            # Hack para colunas longas (Texto completo)
                            if 'INFORMA√á√ÉO' in resultado_df.columns:
                                st.markdown("##### Informa√ß√£o Detalhada (Texto Completo):")
                                
                                # Cria a tabela Markdown (T√≠tulo e Corpo)
                                table_markdown = "| | INFORMA√á√ÉO |\n"
                                table_markdown += "| :--- | :--- |\n"
                                
                                # Adiciona as linhas do DataFrame
                                for index, row in resultado_df.iterrows():
                                    # Usa o √≠ndice como a primeira coluna e o texto como a segunda
                                    index_display = index if resultado_df.index.name is None else row.name
                                    table_markdown += f"| **{index_display}** | {row['INFORMA√á√ÉO']} |\n"
                                
                                st.markdown(table_markdown)
                                
                            else:
                                # Usa o st.dataframe normal para colunas num√©ricas/curtas
                                column_config = {col: st.column_config.Column(
                                    width="large",
                                    help="Descri√ß√£o"
                                ) for col in resultado_df.columns}

                                if resultado_df.index.name:
                                    column_config[resultado_df.index.name] = st.column_config.TextColumn(
                                        width="small",
                                        help="Tipo/√çndice"
                                    )
                                st.dataframe(resultado_df, use_container_width=True, column_config=column_config)
                        
                        # Exibe Gr√°fico (se houver)
                        if img_bytes and 'habilitar_grafico' not in st.session_state:
                            st.subheader("Gr√°fico Gerado:")
                            st.image(img_bytes, caption="Gr√°fico da An√°lise", use_container_width=True)
    
    # --- L√≥gica de Exibi√ß√£o dos Bot√µes Secund√°rios ---
    
    if 'exibir_codigo' in st.session_state and st.session_state['exibir_codigo']:
        st.session_state['exibir_codigo'] = False
        if st.session_state.get('codigo_gerado'):
            st.subheader("C√≥gido Python Gerado:")
            st.code(st.session_state['codigo_gerado'], language='python')
        else:
            st.warning("Nenhum c√≥digo gerado. Fa√ßa uma consulta primeiro.")

    if 'habilitar_grafico' in st.session_state and st.session_state['habilitar_grafico']:
        st.session_state['habilitar_grafico'] = False
        if st.session_state.get('img_bytes'):
            st.subheader("Gr√°fico Gerado:")
            st.image(st.session_state['img_bytes'], caption="Gr√°fico da An√°lise", use_container_width=True)
        else:
            st.warning("Nenhum gr√°fico gerado na √∫ltima consulta.")

    if 'gerar_pdf' in st.session_state and st.session_state['gerar_pdf']:
        st.session_state['gerar_pdf'] = False
        if st.session_state.get('codigo_gerado'):
            with st.spinner("Gerando PDF..."):
                _, _, pdf_bytes = agente3_formatar_apresentacao(st.session_state['resultado_texto'], st.session_state.get('resultado_df'), pergunta, st.session_state.get('img_bytes'))
                
                if pdf_bytes:
                    st.subheader("Download do Relat√≥rio:")
                    st.download_button(
                        label="Baixar Relat√≥rio em PDF",
                        data=pdf_bytes,
                        file_name="relatorio_eda.pdf",
                        mime="application/pdf"
                    )
                else:
                    st.warning("N√£o foi poss√≠vel gerar o PDF. Verifique se as bibliotecas (ReportLab) est√£o instaladas.")
        else:
            st.warning("Por favor, execute uma consulta e analise os dados primeiro para gerar o PDF.")

st.markdown("---")

# --- Se√ß√£o 3: Conclus√µes do Agente ---
st.header("3. Conclus√µes do Agente")
if st.session_state['conclusoes_historico']:
    st.markdown(st.session_state['conclusoes_historico'])
else:
    st.info("As conclus√µes da an√°lise aparecer√£o aqui ap√≥s a primeira consulta.")

st.markdown("---")
st.markdown("Adaptado por Rafael - grupo TENKAI")
